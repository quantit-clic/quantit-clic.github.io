---
layout: default
description: ''
---

# Team

<table class='organizer-pics-four'>
    <tr>
        <td>
        <img class='im-speaker-pic' src='images/sandro.jpg' alt='sandro'>
        </td>
        <td>
        <img class='im-speaker-pic' src='images/ionut.jpg' alt='ionut'>
        </td>
        <td>
        <img class='im-speaker-pic' src='images/aurelie.jpg' alt='aurelie'>
        </td>
        <td>
        <img class='im-speaker-pic' src='images/raffa.jpg' alt='raffa'>
        </td>
    </tr>

    <tr>
        <td><a href='http://sandropezzelle.github.io/'>Sandro Pezzelle</a> <br> CIMeC, University of Trento </td>

        <td><a href=''>Ionut-Teodor Sorodoc</a> <br>
        Universitat Pompeu Fabra </td>

        <td><a href='http://aurelieherbelot.net/'>Aurelie Herbelot</a> <br>
        Universitat Pompeu Fabra </td>

        <td> <a href='http://disi.unitn.it/~bernardi/'>Raffaella Bernardi</a> <br>
        CIMeC, DISI, University of Trento</td>
    </tr>
</table>

<br>
<br>
<br>

# Comparatives, Quantifiers, Proportions: A Multi-Task Model for the Learning of Quantities from Vision

##<em>NAACL-HLT 2018</em><br>[pdf] [bib] [data] [code] [slides]

<div style="text-align: justify">
<p><b>Pezzelle, S., Sorodoc, I., Bernardi, R.</b></p>
<p>The present work investigates whether different quantification mechanisms (set comparison, vague quantification, and proportional estimation) can be jointly learned from visual scenes by a multi-task computational model. The motivation is that, in humans, these processes underlie the same cognitive, non-symbolic ability, which allows an automatic estimation and comparison of set magnitudes. We show that when information about lower-complexity tasks is available, the higher-level proportional task becomes more accurate than when performed in isolation. Moreover, the multi-task model is able to generalize to unseen combinations of target/non-target objects. Consistently with behavioral evidence showing the interference of absolute number in the proportional task, the multi-task model no longer works when asked to provide the number of target objects in the scene.</p></div>

<br>

<div id="proj-pic" style="text-align:center;">
    <p>
    <a href="">
    <img class="fblogo" border="0" style="display: inline-block; margin-right: auto; height: 480px; margin-left: auto;" src="images/prova.png"/></a>
    </p>
    
</div>

<br>
<br>
<br>

# Learning Quantification from Images: A Structured Neural Architecture

##<em>Journal Natural Language Engineering 2018</em><br>[pdf] [bib] [data] [code]

<div style="text-align: justify">
<p><b>Sorodoc, I., Pezzelle, S., Herbelot, A., Dimiccoli, M., Bernardi, R.</b></p>
<p>Major advances have recently been made in merging language and vision representations.
Most tasks considered so far have confined themselves to the processing of objects and
lexicalised relations amongst objects (content words). We know, however, that humans
(even pre-school children) can abstract over raw multimodal data to perform certain types
of higher-level reasoning, expressed in natural language by function words. A case in point
is given by their ability to learn quantifiers, i.e. expressions like few, some and all.
From formal semantics and cognitive linguistics we know that quantifiers are relations
over sets which, as a simplification, we can see as proportions. For instance, in most fish
are red, most encodes the proportion of fish which are red fish. In this paper, we study
how well current neural network strategies model such relations. We propose a task where,
given an image and a query expressed by an object-property pair, the system must return
a quantifier expressing which proportions of the queried object have the queried property.
Our contributions are twofold. First, we show that the best performance on this task involves coupling state-of-the-art attention mechanisms with a network architecture mirroring the logical structure assigned to quantifiers by classic linguistic formalisation. Second,
we introduce a new balanced dataset of image scenarios associated with quantification
queries, which we hope will foster further research in this area.</p></div>

<br>

<div id="proj-pic" style="text-align:center;">
    <p>
    <a href="">
    <img class="fblogo" border="0" style="display: inline-block; margin-left: 0px; height: 360px; margin-right: auto;" src="images/COCO-1.jpg"/></a>
    </p>
</div>

<br>
<br>
<br>


# Be Precise or Fuzzy: Learning the Meaning of Cardinals and Quantifiers from Vision

##<em>EACL 2017</em><br>[pdf] [bib] [data] [code] [poster]

<div style="text-align: justify">
<p><b>Pezzelle, S., Marelli, M., Bernardi, R.</b></p>
<p>People can refer to quantities in a visual scene
by using either exact cardinals (e.g. one, two,
three) or natural language quantifiers (e.g. few,
most, all). In humans, these two processes
underlie fairly different cognitive and neural
mechanisms. Inspired by this evidence, the
present study proposes two models for learning the objective meaning of cardinals and
quantifiers from visual scenes containing multiple objects. We show that a model capitalizing on a ‘fuzzy’ measure of similarity is effective for learning quantifiers, whereas the learning of exact cardinals is better accomplished
when information about number is provided. </p></div>

<br>

<div id="proj-pic" style="text-align:center;">
    <p>
    <a href="">
    <img class="fblogo" border="0" style="display: inline-block; margin-left: 0px; margin-bottom: 50px; height: 200px; margin-right: auto;" src="images/dogscats.jpg"/></a>
    <a href="">
    <img class="fblogo" border="0" style="display: inline-block; margin-right: auto; height: 300px; margin-left: auto;" src="images/diagram4.png"/></a>
    </p>

</div>

<br>

<div style="text-align: justify">
<code>
@InProceedings{pezzelle2017,<br>
&nbsp;&nbsp;author    = {Pezzelle, Sandro  and  Marelli, Marco  and  Bernardi, Raffaella},<br>
&nbsp;&nbsp;title     = {Be Precise or Fuzzy: Learning the Meaning of Cardinals and Quantifiers from Vision},<br>
&nbsp;&nbsp;booktitle = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers},<br>
&nbsp;&nbsp;month     = {April},<br>
&nbsp;&nbsp;year      = {2017},<br>
&nbsp;&nbsp;address   = {Valencia, Spain},<br>
&nbsp;&nbsp;publisher = {Association for Computational Linguistics},<br>
&nbsp;&nbsp;pages     = {337--342}<br>
}
</code>
</div>

<br>
<br>
<br>


# "Look, Some Green Circles!": Learning to Quantify from Images

<em>ACL Workshop on Vision and Language 2016</em><br>[pdf] [bib] [data] [code] [poster]

<div style="text-align: justify">
<p><b>Sorodoc, I., Lazaridou, A., Boleda, G., Herbelot, A., Pezzelle, S., Bernardi, R.</b></p>
<p>In this paper, we investigate whether a
neural network model can learn the meaning of natural language quantifiers (no,
some and all) from their use in visual contexts. We show that memory networks perform well in this task, and that explicit
counting is not necessary to the system’s
performance, supporting psycholinguistic
evidence on the acquisition of quantifiers. </p></div>

<br>

<div id="proj-pic" style="text-align:center;">
    <p>
    <a href="">
    <img class="fblogo" border="0" style="display: inline-block; margin-right: auto; height: 360px; margin-left: auto;" src="images/diagram-dots.jpg"/></a>
    </p>

</div>

<br>

<div style="text-align: justify">
<code>
@inproceedings{sorodoc2016look,<br>
&nbsp;&nbsp;title={“Look, some green circles!”: Learning to quantify from images},<br>
&nbsp;&nbsp;author={Sorodoc, Ionut and Lazaridou, Angeliki and Boleda, Gemma and Herbelot, Aur{\'e}lie and Pezzelle, Sandro and Bernardi, Raffaella},<br>
&nbsp;&nbsp;booktitle={Proceedings of the 5th Workshop on Vision and Language},<br>
&nbsp;&nbsp;pages={75--79},<br>
&nbsp;&nbsp;year={2016}<br>
}
</code>
</div>

<br>
<br>
<br>
